<h1 id="introduction"><span class="header-section-number">1</span> Introduction</h1>
<p>(<strong>Note:</strong> the quality and the content of the text is not yet reflective of whatever I have in mind for my thesis)</p>
<p>Although various machine learning approaches to numerical determination and estimation of objects in images already exist, some research seems impartial to broader cognitive debate (see Stoianov and Zorzi 2012), resulting in models that are somewhat unhelpful for progressing the understanding of numerical cognition.</p>
<p>Visual numerical cognition functions differently to for example mathematical/arithmetical numerical cognition in regard to three closely related principles (properties). These properties consequently guide the general approach of this research (source? maybe note that I can discern three, lakoff might be an source)</p>
<ol type="1">
<li>Firstly, ‚Ä¶</li>
<li>Fusce commodo.</li>
<li>Visual sense of number is an emergent property of neurons embedded in generative hierarchical learning processes/models. (biological/animal stuff) (Artificial networks) Modeling visual number therefore necessitates non-researcher depended(/i.e.¬†handcrafted (you could I guess cite dreyfus that this is an unrealistic way of learning that AI suffers from)) features, restricting the choice of an algorithm to be unsupervised as such an algorithm will learn . Given their ability to construct the underlying (stochastic?) representation of the data distribution(they learn their own features?) (‚Ä¶in an unsupervised manner), <em>Varitional Autoencoders</em> (VAEs) seem fit to tackle this problem.</li>
</ol>
<p><br /><span class="math display">$$
\frac{n!}{m!(n-m)!} = {n \choose m}
$$</span><br /></p>
<h1 id="related-work"><span class="header-section-number">2</span> Related Work</h1>
<h2 id="visual-number-sense"><span class="header-section-number">2.1</span> Visual Number Sense</h2>
<h2 id="subitizing"><span class="header-section-number">2.2</span> Subitizing</h2>
<!-- First also talk about how subitizing is a type of visual number sense, what it is, what the subitizing range is etc. -->
<!-- Also mention something about how this dataset is concustructed -->
<!-- Do you also explain synthetic data here? (see original ordering) -->
<p>As constructing a dataset fit for visual numerosity estimation is a difficult task given the lack of other datasets made out of natural images containing a variety of labeled, large object groups, we set out to model the phenomenon of <em>subitizing</em>, a type of visual number sense which had a dataset catered to this phenomenon readily avialable. As seen in the <a href="#sub">figure</a> below, the goal of the <em>Salient Object Subitizing</em> (SOS) dataset as defined by Zhang et al. (2016) is to clearly show a number of salient objects that lies within the subitizing range.</p>
<figure>
<img src="https://github.com/rien333/numbersense-vae/blob/master/thesis/subitizing.png" title="Example images from the SOS dataset" alt="sos_example" id="fig:sub" /><figcaption>sos_example</figcaption>
</figure>
<h1 id="methods"><span class="header-section-number">3</span> Methods</h1>
<h2 id="variational-autoencoder"><span class="header-section-number">3.1</span> Variational Autoencoder</h2>
<p>Optimization objectives</p>
<ol type="1">
<li><span class="math inline">ùí¶‚Ñí[ùí©(<em>Œº</em>(<em>X</em>),‚ÄÜ<em>Œ£</em>(<em>X</em>))||ùí©(0,‚ÄÜ<em>I</em>)]</span></li>
<li>Visual reconstruction loss (e.g.¬†BCE)</li>
</ol>
<h2 id="deep-feature-consistent-perceptual-loss"><span class="header-section-number">3.2</span> Deep Feature Consistent Perceptual Loss</h2>
<p>To make the reconstructions made by the VAE perceptually closer to whatever humans deem important characteristics of images, Hou et al. (2017) propose optimizing the reconstructions with help of the hidden layers of a pretrained network. This can be done by predefining a set of layers <span class="math inline"><em>l</em><sub><em>i</em></sub></span> from a pretrained network (Hou et al. (2017) and present research use VGG-19), and for every <span class="math inline"><em>l</em><sub><em>i</em></sub></span> matching the hidden representation of the input <span class="math inline"><em>x</em></span> to the hidden representation of the reconstruction <span class="math inline"><em>xÃÑ</em></span> made by the VAE:</p>
<p><br /><span class="math display">‚Ñí<sub><em>r</em><em>e</em><em>c</em></sub><sup><em>l</em><sub><em>i</em></sub></sup>‚ÄÑ=‚ÄÑMSE(<em>Œ¶</em>(<em>x</em>)<sup><em>l</em><sub><em>i</em></sub></sup>,‚ÄÜ<em>Œ¶</em>(<em>xÃÑ</em>ÃÑ)<sup><em>l</em><sub><em>i</em></sub></sup>)</span><br /></p>
<p>The more mathematical intuition behind this loss is that whatever some hidden layer <span class="math inline"><em>l</em><sub><em>i</em></sub></span> of the VGG-19 network encodes should be retained in the reconstructed output, as the VGG-19 has proven to model important visual characteristics of a large variety of image types (VGG-19 having been trained on ImageNet).</p>
<h1 id="experiments"><span class="header-section-number">4</span> Experiments</h1>
<h2 id="hidden-representation-classifier"><span class="header-section-number">4.1</span> Hidden Representation Classifier</h2>
<h3 id="classifierarch"><span class="header-section-number">4.1.1</span> Classifier architecture</h3>
<p>To asses wether the learned latent space of the VAE network supports subitizing/(the VAE) showcases the (emergent) ability to perform in subitizing, a two layer fully-connected net is fed with latent activation vectors <span class="math inline"><em>z</em><sub><em>X</em><sub><em>i</em></sub></sub></span> created by the encoder module of the VAE from an image <span class="math inline"><em>X</em><sub><em>i</em></sub></span>, and a corresponding subitizing class label <span class="math inline"><em>Y</em><sub><em>i</em></sub></span>, where <span class="math inline"><em>X</em><sub><em>i</em></sub></span> and <span class="math inline"><em>Y</em><sub><em>i</em></sub></span> are respectively an image and class label from the SOS training set. Both fully-connected layers contain 160 neurons. Each of the linear layers is followed by batchnorm layer (<span class="citeproc-not-found" data-reference-id="batchnorm-ref"><strong>???</strong></span>), a ReLU activation function and a dropout layer, respectively. A fully-connected net was choosen because using another connectionist module for read-outs of the hidden representation heightens the biological plausibility of the final approach (Zorzi, Testolin, and Stoianov 2013). Zorzi, Testolin, and Stoianov (2013) note that the appended connectionist classifier module can for example be concieved of as cognitive response module (?), although the main reason behind training this classifier is to asses it‚Äôs performance against other algorithmic and human data.</p>
<h3 id="class-imbalance"><span class="header-section-number">4.1.2</span> Class imbalance</h3>
<p>Class imbalance is a phenomenon sometimes encountered in datasets whereby the number of instances belonging to one or more of the classes is significantly higher than the amount of instances belonging to any of the other classes. Although exact definitions on what constitues class imbalance are highly depended on the problem and method, we follow (<span class="citeproc-not-found" data-reference-id="ref"><strong>???</strong></span>) in that the most represented classes should at least account for 40% of the dataset size. For the SOS dataset, this implies that class 0 and 1 will <em>majority classes</em>, while the others should be considered <em>minority classes</em>. Notably, class imbalance can not only be concieved of in terms of quantitative difference, but also in qualitive difference, whereby the relative importance of some class is higher than others (e.g.¬†in classification relating to malignent tumors, misclassifying malignent examples as unmalignent could be weighted more strongly, see (<span class="citeproc-not-found" data-reference-id="ref"><strong>???</strong></span>)). Most literature makes a distinction between three kinds of aproaches to taclkle class imbalance (for a discussion see <span class="citeproc-not-found" data-reference-id="imbalance"><strong>???</strong></span>, <span class="citation" data-cites="ADASyn">@ADASyn</span> <span class="citation" data-cites="imbalance2">@imbalance2</span>).</p>
<ol type="1">
<li><em>Oversampling techniques</em>.</li>
<li><em>Undersampling techniques</em>.</li>
<li><em>Mauris mollis tincidunt felis.</em></li>
</ol>
<p>An ensemble of techniques was used to tackle the class imbalance in the SOS dataset. First, slight random undersampling of the two majority classes (0 and 1) is performed, reducing their size by ~10% (<span class="citeproc-not-found" data-reference-id="ref-random-undersample"><strong>???</strong></span>, sckikit maybe). Furthermore, as in practice many common sophisticated under- and oversampling techniques (e.g.¬†data augmentation or outlier removal, for an overview see (<span class="citeproc-not-found" data-reference-id="imbalance"><strong>???</strong></span>)) proved non-effective, another so called &quot;&quot; algorith was used, namely cost-sensitive class weighting. Cost-senstive ‚Ä¶ consists of ‚Ä¶. The uneffectiveness of quantive sampling techniques is likely to be caused by that in addition to the quantitative difference in class examples, there is also a slight difficulty factor whereby assesing the class of latent vector <span class="math inline"><em>z</em></span> is significantly if belongs to class 2 or 3 versus any other, for these two classes require rather precise contours to discern the invidual objects, in case of them for example overlapping, which remains hard for VAEs given their tendency to produce blurred reconstructions. The classifier network therefore seems inclined to put all of its representational power towards the easier classes, as this will result in a lower total cost, whereby this inclination will become even stronger as the quantitative class imbalance grows. The class weights for cost sensitive learning are set according to the quantitative class imbalance ratio (<span class="citeproc-not-found" data-reference-id="ref"><strong>???</strong></span>), but better accuracy was obtained by slightly altering the relative difference between the weight by raising all of them to some power <span class="math inline"><em>n</em></span>. In our experiments, <span class="math inline"><em>n</em>‚ÄÑ=‚ÄÑ3</span> resulted in a balance between high per class accuray scores and aforementioned scores roughly following the same shape as in other algorithms, which hopefully implies that the classifier is able to generalize in a manner comparable to previous approaches. For the SOS dataset with random majority class undersampling, if <span class="math inline"><em>n</em>‚ÄÑ‚â´‚ÄÑ3</span> the classifier accuracy for the majority classes shrinks towards chance, and, interestingly, accuracy for the minority classes becomes comparable to the state of the art.</p>
<h1 id="results-discussion"><span class="header-section-number">5</span> Results &amp; Discussion</h1>
<h2 id="subitizing-read-out"><span class="header-section-number">5.1</span> Subitizing Read-Out</h2>
<!-- TODO
    - [ ] Mention something about what other alogithms do -->
<p>Accuray of the <code>zclassifier</code> (i.e.¬†the classifier as described in <a href="#classifierarch"><strong>section x.x</strong></a> that learns to classify latent activation patterns to subitizing labels) is reported over the witheld SOS test set. Accuracy scores of other algorithms were copied over from Zhang et al. (2016).</p>
<table>
<thead>
<tr class="header">
<th style="text-align: right;"></th>
<th>0</th>
<th>1</th>
<th>2</th>
<th>3</th>
<th>4+</th>
<th>mean</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: right;">Chance</td>
<td>27.5</td>
<td>46.5</td>
<td>18.6</td>
<td>11.7</td>
<td>9.7</td>
<td>22.8</td>
</tr>
<tr class="even">
<td style="text-align: right;">SalPry</td>
<td>46.1</td>
<td>65.4</td>
<td>32.6</td>
<td>15.0</td>
<td>10.7</td>
<td>34.0</td>
</tr>
<tr class="odd">
<td style="text-align: right;">GIST</td>
<td>67.4</td>
<td>65.0</td>
<td>32.3</td>
<td>17.5</td>
<td>24.7</td>
<td>41.4</td>
</tr>
<tr class="even">
<td style="text-align: right;">SIFT+IVF</td>
<td>83.0</td>
<td>68.1</td>
<td>35.1</td>
<td>26.6</td>
<td>38.1</td>
<td>50.1</td>
</tr>
<tr class="odd">
<td style="text-align: right;">zclasifier</td>
<td>76</td>
<td>49</td>
<td>40</td>
<td>27</td>
<td>30</td>
<td>44.4</td>
</tr>
<tr class="even">
<td style="text-align: right;">CNN_FT</td>
<td>93.6</td>
<td>93.8</td>
<td>75.2</td>
<td>58.6</td>
<td>71.6</td>
<td>78.6</td>
</tr>
</tbody>
</table>
<!-- # | zclasifier | 73   | 49   | 37   | 31   | 26   | 49   | -->
<!-- Epoch 67 -> Test Accuracy: 51 % -->
<!-- Accuracy of     0 : 78 % -->
<!-- Accuracy of     1 : 53 % -->
<!-- Accuracy of     2 : 35 % -->
<!-- Accuracy of     3 : 25 % -->
<!-- Accuracy of     4 : 28 % -->
<!-- Epoch 45 -> Test set loss: 34.28524211883544837 -->
<!-- Mean Accuracy     : 49 % -->
<!-- Accuracy of     0 : 68 % -->
<!-- Accuracy of     1 : 52 % -->
<!-- Accuracy of     2 : 35 % -->
<!-- Accuracy of     3 : 23 % -->
<!-- Accuracy of     4 : 31 % -->
<p>The subitizing performance of the VAE is comparable to highest scoring non-machine learning algorithm, and performs worse overall than the CNNs trained by Zhang et al. (2016). This can be explained by a number of factors. First of all, the <code>CNN_ft</code> algorithm used by Zhang et al. (2016) has been pretrained on the large, well tested databese of images it is trained on (i.e.¬†ImageNet, which contains N images, while our procedure uses M syntethic and N2 natural images). Additionaly, their model is capable of more complex representations due its depth and the amount of modules it contains (). Moreover, all their alogirhtms are trained in a supervised manner, which can sometimes be a lot easier than unsupervised training (<span class="citeproc-not-found" data-reference-id="ref"><strong>???</strong></span>)</p>
<h2 id="qualitive-analysis"><span class="header-section-number">5.2</span> Qualitive Analysis</h2>
<h1 id="conclusion"><span class="header-section-number">6</span> Conclusion</h1>
<h1 id="references" class="unnumbered">References</h1>
<div id="refs" class="references">
<div id="ref-hou2017deep">
<p>Hou, Xianxu, Linlin Shen, Ke Sun, and Guoping Qiu. 2017. ‚ÄúDeep Feature Consistent Variational Autoencoder.‚Äù In <em>Applications of Computer Vision (Wacv), 2017 Ieee Winter Conference on</em>, 1133‚Äì41. IEEE.</p>
</div>
<div id="ref-Stoianov2012">
<p>Stoianov, Ivilin, and Marco Zorzi. 2012. ‚ÄúEmergence of a‚Äôvisual Number Sense‚Äôin Hierarchical Generative Models.‚Äù <em>Nature Neuroscience</em> 15 (2). Nature Publishing Group: 194.</p>
</div>
<div id="ref-zhang2016salient">
<p>Zhang, Jianming, Shuga Ma, Mehrnoosh Sameki, Stan Sclaroff, Margrit Betke, Zhe Lin, Xiaohui Shen, Brian Price, and Radom√≠r Mƒõch. 2016. ‚ÄúSalient Object Subitizing.‚Äù <em>arXiv Preprint arXiv:1607.07525</em>.</p>
</div>
<div id="ref-zorzi2013modeling">
<p>Zorzi, Marco, Alberto Testolin, and Ivilin Peev Stoianov. 2013. ‚ÄúModeling Language and Cognition with Deep Unsupervised Learning: A Tutorial Overview.‚Äù <em>Frontiers in Psychology</em> 4. Frontiers: 515.</p>
</div>
</div>
