%% TODO
% - [x] some of the intro can be moved to related work, but shorter
% 		- Read up on other related works
%			- is it okay to motivate most of your approach in the related work section?
% 			- yes
% 		- make a section about computational models and visual number sense in general to motivate your 
% 		approach
% 			- but what will it contain?
% 				- CNNs
% 				- scale/visual feature invarience? (independt and dedicated to numerisoty, no arabic numerals)
% 					- This indepedence is assumed because...
% 				- niet gebruiken van andere techniecken dan visuele troep (sluit aan op vorige)
% - [x] Have al figures named "Fig." and refer to them as such
% - [x] abstract
% - [x] Cite robolab (this is kinda mandatory, acknowledgements?)
% - [ ] Subtitle?
% - [x] Add training parameters section (bezier, prepossecing, VAE architecture)
% - [x] You could decide to say we trained our VAE for blah blah (in pytorch)
% - [ ] If appropiate, make section headings a little bit small (and then subsubsection not bold?)

\PassOptionsToPackage{unicode=true}{hyperref} % options for packages loaded elsewhere
\PassOptionsToPackage{hyphens}{url}
\PassOptionsToPackage{dvipsnames,svgnames*,x11names*}{xcolor}
%
\documentclass[twocolumn]{article}
\usepackage{lmodern}
\usepackage{amssymb,amsmath}
\usepackage{ifxetex,ifluatex}
\usepackage{fixltx2e} % provides \textsubscript
\ifnum 0\ifxetex 1\fi\ifluatex 1\fi=0 % if pdftex
  \usepackage[T1]{fontenc}
  \usepackage[utf8]{inputenc}
  \usepackage{textcomp} % provides euro and other symbols
\else % if luatex or xelatex
  \usepackage{unicode-math}
  \defaultfontfeatures{Ligatures=TeX,Scale=MatchLowercase}
\fi
% use upquote if available, for straight quotes in verbatim environments
\IfFileExists{upquote.sty}{\usepackage{upquote}}{}
% use microtype if available
\IfFileExists{microtype.sty}{%
\usepackage[]{microtype}
\UseMicrotypeSet[protrusion]{basicmath} % disable protrusion for tt fonts
}{}
%% \IfFileExists{parskip.sty}{%
%% \usepackage{parskip}
%% }{% else
%% \setlength{\parindent}{0pt}
%% \setlength{\parskip}{6pt plus 2pt minus 1pt}
%% }

\usepackage{xcolor}
\usepackage{hyperref}
\hypersetup{
            pdftitle={Modeling Subitizing with Variational Autoencoders},
            colorlinks=true,
            linkcolor=Maroon,
            citecolor=Blue,
            urlcolor=cyan,
            breaklinks=true}
\urlstyle{same}  % don't use monospace font for urls
%% \usepackage[margin=0.42in, bottom=0.5in]{geometry}
\usepackage[margin=0.68in]{geometry}
\usepackage{longtable,booktabs}
% Fix footnotes in tables (requires footnote package)
\IfFileExists{footnote.sty}{\usepackage{footnote}\makesavenoteenv{longtable}}{}
\usepackage{graphicx,grffile}
\makeatletter
\def\maxwidth{\ifdim\Gin@nat@width>\linewidth\linewidth\else\Gin@nat@width\fi}
\def\maxheight{\ifdim\Gin@nat@height>\textheight\textheight\else\Gin@nat@height\fi}
\makeatother
% Scale images if necessary, so that they will not overflow the page
% margins by default, and it is still possible to overwrite the defaults
% using explicit options in \includegraphics[width, height, ...]{}
\setkeys{Gin}{width=\maxwidth,height=\maxheight,keepaspectratio}
\setlength{\emergencystretch}{3em}  % prevent overfull lines
\providecommand{\tightlist}{%
  \setlength{\itemsep}{0pt}\setlength{\parskip}{0pt}}
\setcounter{secnumdepth}{5}
% Redefines (sub)paragraphs to behave more like sections
\ifx\paragraph\undefined\else
\let\oldparagraph\paragraph
\renewcommand{\paragraph}[1]{\oldparagraph{#1}\mbox{}}
\fi
\ifx\subparagraph\undefined\else
\let\oldsubparagraph\subparagraph
\renewcommand{\subparagraph}[1]{\oldsubparagraph{#1}\mbox{}}
\fi


% set default figure placement to htbp
\makeatletter
\def\fps@figure{htbp}
\makeatother

\newcommand\inlineeqno{\stepcounter{equation}\ (\theequation)}
\renewcommand\figurename{Fig.}

\renewenvironment{abstract}
               {\list{}{\leftmargin}%
                \item[\hspace{7mm}\textbf{Abstract}]\relax}
               {\endlist}

\setlength{\columnsep}{0.32in}

%% \setlength{\parindent}{0pt}
%% \setlength{\parskip}{\baselineskip}
\setlength{\parindent}{1.6em}

\usepackage{float}
\usepackage{titlesec}
\usepackage{subcaption}
%% \usepackage{subfig}
\usepackage[tableposition=top]{caption}
\usepackage{enumitem}
\usepackage{amssymb}
\usepackage{amsmath}
\usepackage[round]{natbib}
\bibliographystyle{unsrtnat}
\usepackage[affil-it]{authblk}


\title{Modeling Subitizing with Variational Autoencoders}
\author{Rijnder Wever \(\cdot\) Tom Runia}
%% \supervisor{Tom Runia}
\affil{University of Amsterdam} % Large gap?
\date{\today}

\begin{document}

\begin{titlepage}
    \begin{center}
        %% \vspace*{1cm}
      	\topskip0pt
        \vspace*{\fill} % Center
        \Huge
        \textbf{Modeling Subitizing with Variational Autoencoders}
        
        \vspace{0.5cm}
        \textit{Bachelor Thesis Subtitle}
        
        \vspace{1.5cm}
        \Large
        \textbf{Rijnder Wever \(\cdot\) Tom Runia (Supervisor)}\\
        
        \vfill
        
        A thesis presented for the degree of\\
        Doctor of Philosophy
        
        \vspace{0.8cm}
        
        \includegraphics[width=0.4\textwidth]{uva}
        
        Artificial Intelligence\\
        Country\\
        \today
        
    \end{center}
\end{titlepage}

\onecolumn % table of contents stuff
{
  \hypersetup{linkcolor=black}
  \topskip0pt
  \vspace*{\fill} % Center
  \tableofcontents
  \section*{Acknowledgements}

  Special thanks to Tom Runia, my bachelor thesis supervisor, for assisting me throughout this project and prodiving me with the nessecary computing facillities. Furthermore, I want to thank the UvA RoboLab for granting me acces to their prototyping machine. Finally, the Lisa Cluster provided me with all the computing power one could ever need, bringing peace of mind whenever I was worried I was using other's resources. 

  \vspace*{\fill}
}
\newpage
\twocolumn

\maketitle


\begin{abstract}
Many animals develop the perceptual ability to \emph{subitize}, allowing them to instantly and accurately assess the number of items in a small group. Such a visual number sense is shown to emerge in hierarchical generative models provided with spatial configurations of homogeneous rectangles in various sizes. A promising recent set of generative algorithms,  \emph{variational autoencoders} (VAEs), might allow training on more complex natural images, invariance to visual complexity being an essential aspect of any abstract sense of number. We show that our unsupervised VAE algorithm is able to spontaneously perform in subitizing judgments when provided with natural images from a dataset catered to subitizing. Although the VAE's subitizing performance is behind some supervised algorithms, it encodes numerical information related to subitizing separately from other visual information, similar to the functioning of neuronal correlates of visual number sense. In particular, we show that the learned subitizing encoding is likely invariant to object area, comparable to how biological networks neglect area information when judging numerosity.
\end{abstract}

\hypertarget{introduction}{%
\section{Introduction}\label{introduction}}

Although various machine learning approaches dealing with the numerical
determination of the amount of objects in images already exist, some
research can be impartial to broader cognitive debate, resulting in
models that are somewhat unhelpful for progressing the understanding of
numerical cognition. Neurological findings about the general characteristics of cognitive processes performing in numerical tasks have successfully been applied to structuring and evaluating computational models of numerical cognition that aim to maintain biological plausibility \citep{stoianov2012, zhang2016salient}.
%% Original
%% Any approach to computational modeling of numerical
%% cognition that aims to maintain biological plausibility should adhere to
%% neurological findings about the general characteristics of cognitive
%% processes related to numerical tasks.
Essential to understanding the
cognitive processes behind number sense is their perceptual origin, for
so called \emph{visual number sense} has been posed as the fundamental
basis for developmentally later kinds of number sense, such as that
required for arithmetical thinking and other more rigorous concepts of
number found in mathematics \citetext{\citealp[ chap.
2]{lakoff}; \citealp{numerosity-basis}}. Visual number sense is the
perceptual capability of many animals to immediately perceive a group of items as
having either a distinct or approximate cardinality, hereafter \emph{numerosity} \citep{feigenson2004core}. 

Numerosity-selective neurons have been shown to spontaneously emerge in dedicated neuronal networks, possibly as a result of numerosity being an integral part of the sensory world \citep{viswanathan2013neuronal}. As such, visual number sense can be said to emerge as a perceptual category in an interplay between numerosity-percepts and neural populations' tuning to some preferred visual numerosity \citep{nieder2016neuronal}. The experientially instantaneous capability to discriminate visual numerosity that many animals share \citep{nieder2016neuronal, animalsnumericalcognition} can thus be explained by it primarily relying on a neuronal coding scheme, so avoiding a dependence on higher-cognitive processes such as conscious representations \citep[p.~58 points to types of visual number sense being pre-attentive]{dehaene2011number} or symbolic processing \citep[visual numerosity percepts are understood non-verbally,][]{nieder2016neuronal}. The response profile of neural populations detecting numerosity was found to be invariant to all visual features except quantity \citep{nieder2016neuronal, harvey2013topographic}.

Similarly, computational models of numerical cognition provided only with sensory-like information  show that an approximate visual sense of number can emerge as a property of hierarchically organized neural populations embedded in a generative learning model \citep{stoianov2012}. An interesting biological motivation for such a model could be that biological numerosity detectors also exhibit hierarchical organization. \citep{nieder2016neuronal, viswanathan2013neuronal}, as well as aforementioned numerosity networks residing in the neo-cortex \citep{nieder2016neuronal}, a brain area with an anatomical structure displaying likeness to hierarchical Bayesian generative models
\citetext{\citealp{friston2003learning}; although the brain in general is sometimes characterized as a probabilistic modeler of it's sensory world, e.g. \citealp{bayesianbrain, griffiths2006optimal}}, resembling \citet{stoianov2012}'s model ability to distribute stochastic representations of sensory input across a multilayered, hierarchical network. Interestingly, the neurocognitively inspired model of \citet{stoianov2012} developed an encoding scheme that shared multiple similarities with the functional characteristics of biological neuronal populations performing in visual numerosity judgments, among which an invariance to occupied visual area when judging numerosity.

Notwithstanding the success of previous biologically informed approaches to modeling numerical cognition with artificial neural networks, \citet{stoianov2012} note that more work is to be done in applying such models to natural images. The main reason behind pursuing natural images is improving the biological plausibility over previous approaches relying on binary images containing only simple geometric shapes \citep[for examples, see][]{stoianov2012, wu2018two, peterson2000computational}, given that natural images are closer to everyday sensations than binary images. Furthermore, any dataset with visually uniform objects does not capture how visual number sense in animals is abstract in regard to the perceived objects \citep{nieder2016neuronal}, implying that a model should be able show that it performs equally well between objects of different visual complexities.

Unfortunately, no dataset fit for visual numerosity estimation tasks
similar to \citet{stoianov2012} satisfied above requirements (sizable
collections of natural images with large and varied, precisely labeled
object groups are hard to construct), forcing present research towards
\emph{subitizing}, a type of visual number sense with a catered
dataset readily available. Subitizing is the ability of many animals to
immediately perceive the number of items in a group without resorting to e.g. enumeration or arithmetic, given that the number of items falls within the
subitizing range of 1-4 \citep{kaufman1949, animalsnumericalcognition}
Aforementioned characteristics of visual sense of
number hold equally well for more distinct types of numerisoty perception such as
subitizing. Similarly, subitizing is suggested to be a parallel
pre-attentive process in the visual system
\citep[p.~57]{dehaene2011number}, the visual system likely relying on
it's ability to recognize holistic patterns for a final subitizing count
\citetext{\citealp{jansen2014role}; \citealp[p.~57]{dehaene2011number}; \citealp{piazza2002subitizing}}.
This means that the ``sudden'' character of subitizing is caused by the
visual system's ability to process simple geometric configurations of
objects in parallel, whereby increasing the size of a group behind the
subitizing range deprives perceiving this group of it's sudden and
distinct numerical perceptual character for this would strain our
parallelization capabilities too much. Groups exceeding the subitizing parallelization threshold force recourse to numerical judgment techniques with a less sudden perceptual character, such as counting by enumeration. Finally, above descriptions of subitizing have a supposed distinct neuronal correlate in the form of capacity-limited multiple object individuation mechanism whose response profile obeys the perceptual subitizing range \citep{poncet2016individuation}, comparable to how the approximate numerosity system functions as an interplay between perception and coding properties of neurons.\\
\indent Apart from the usage of natural images, present study will further improve biological plausibility by restricting the class of algorithm based on findings about visual sense of number. One such constraint is that an ability to perceive numerosity is shown to emerge in hierarchically organized neural networks embedded in generative learning models, either artificial or biological (as discussed above). Furthermore, the fact that visual number sense exists across species \citep{animalsnumericalcognition}, human newborns newborns \citep[chap. 1]{lakoff} and innumerate cultures \citetext{\citealp[p.~261]{dehaene2011number}; \citealp{franka2008number}} provides strong evidence for the claim that it is an implicitly developed skill at lower cognitive levels, given aforementioned groups lack of exposure to numerical training. Deemed as a general unrealistic trope of artificial learning \citep{dreyfus2007heideggerian} and research into the human learning process \citep{Zorzi2013}, modeling visual number sense therefore necessitates non-researcher depended features. This will restrict the choice of algorithm to so called \emph{unsupervised} learning algorithms, as such an algorithm will learn its own particular representation of the data distribution. Given their ability to infer the underlying stochastic representation of the data in an unsupervised manner, i.e.~perform in autonomous feature determination, \emph{Variational Autoencoders} (VAEs) seem fit to tackle this problem, additional to VAEs being a generative algorithm (section \ref{vae} details their precise working). Moreover, VAEs are trained fully unsupervised similar to how, given appropriate circumstances, visual numerosity related abilities are implicitly learned skills that emerge without ``labeled data''. Another interesting aspect of VAEs is their relatively interpretable and overseeable learned feature space, which might tell something about how it deals with visual numerosity, and thus allows to evaluate the properties of the VAE's encoding against biological data.

Present study therefore asks: how can neural networks provided with natural images be applied within biologically informed models to develop the emergent neuronal skill of subitizing in a manner comparable to their biological equivalents? To answer this, we start % section numbers chronological
with discussing a dataset constructed for modeling subitizing in section \ref{related-work}, and then detail how we implemented our VAE algorithm to learn a representation of this dataset in \ref{vae}.  Slight problems during training and evaluation that arose from properties of our applied dataset and algorithm and appropriate solutions are examined in section \ref{deep-feature-consistent-perceptual-loss}, \ref{hybrid}, and \ref{imbalance}.  Next, as the subitizing task is essentially an image classification task, a full
methodology for evaluating the unsupervised VAE model's performance on
the subitizing classification task is described in section \ref{readout}. Subitizing performance is reported in section \ref{subitizing-read-out}. Finally, section \ref{qualitative-analysis} highlights measurements of the final models robustness to changes in visual features.\\ \\
\noindent The contributions of our work are the following:
\begin{enumerate}[noitemsep,]
\item An unsupervised training setup in which a variational autoencoder algorithm spontaneously develops an ability to subitize is described. Our unsupervised approach seemed to require some specific countermeasures against problematic properties of the applied subitizing dataset, namely class imbalance and data scarcity. Although their appliance proved only partly successful, further investigation could theoretically improve results. 
\item We demonstrate that the
performance of our unsupervised approach is comparable with supervised
approaches using handcrafted features, although performance is still
behind state of the art supervised machine learning approaches due to
problems inherent to the particular VAE implementation. 
\item Measuring the final models robustness to changes in visual features
shows the emergence of a encoding property similar to biological neurons, that is
to say, the VAE's encoding scheme supports numerosity
percepts invariant to visual features other than quantity.
\end{enumerate}

\hypertarget{related-work}{%
\section{Related Work}\label{related-work}}

\newcommand{\relatedss}[2]{{\bfseries #1. }{#2}}

% okay but don't you have two sections doing the same thing now
% Properties of Visual Number Sense
\relatedss{Visual Number Sense}{Important characteristics of numerosity perception inspire algorithm architecture, evaluative measures and the  training setup of present study. Firstly, visual sense of number is an \emph{automatic} appreciation of the sensory world. It can be characterized as ``sudden'', or as visible at a glance \citetext{\citealp[p.~57]{dehaene2011number}; \citealp{zhang2016salient}}. \emph{Convolutional neural networks} (CNNs) not only showcase excellent performance in extracting visual features from complex natural images \citetext{\citealp{GoogleDeepMind}; \citealp{krizhevsky2012imagenet}; \citealp[for visual number sense and CNNs see][]{zhang2016salient}}, but are furthermore functionally inspired by the visual cortex of animals \citep[specifically cats, see][]{lecun1995convolutional}. CNNs mimicking aspects of the animal visual cortex thus make them an appropriate candidate for modeling biologically plausible automatic neural coding of numerosity percepts. 
Secondly, the directness of visual numerosity entails that it does not require the interposition of external processes, or at least no other than lower-level sensory neural processes. For example, numerosity perception has been shown to function independently from mathematical knowledge of number, as numerosity detecting neuronal populations do not display their characteristic response profiles when confronted with Arabic numerals \citep{harvey2013topographic, poncet2016individuation}. This independence partly motivates the assessment of numerisoty detectors solely encoding visual numerisoty information. Visual number sense being a immediate and purely perceptual process implies that our model should not apply external computational techniques sometimes used in computer vision research on numerical determination task such as counting-by-detection \citep[which requires both arithmetic and iterative attention to all group members, see][]{zhang2016salient, detection2016unconstrained} or segmenting techniques \citep[e.g.][]{chattopadhyay2016counting}. Instead, we want the model to operate in an autonomous and purely sensory fashion.}

\relatedss{Modeling approximate numerosity perception}{\citet{stoianov2012} apply neural networks to learning visual numerosity estimation, although without using natural
images. \citet{stoianov2012} discovered neural populations concerned with numerosity
estimation that shared multiple properties with biological populations
participating in similar tasks, most prominently an encoding scheme that
was invariant to the cumulative surface area of the objects present in
the provided images. Present study hopes to discover a similar kind
of invariance to surface area. Likewise, we will employ the same surface area
invariance test (section \ref{qualitative-analysis}), although a successful application to natural images
already implies a fairly abstract representation of number, as the objects
therein are of varying visual complexity.
Some simplicity of the dataset used by \citet{stoianov2012} is due their
use of the relatively computationally expensive Restricted Boltzmann
Machine (RBM) \citep[except when exploiting prior knowledge of
regularities in the probability distribution over the observed data,
equation (20.6) from][ shows that computational cost in RBMs
grows as a multiple of the size of it's hidden and observed
units]{goodfellow2016deep}. Given developments in generative algorithms
and the availability of more computational power, we will therefore opt
for a different algorithmic approach (see section \ref{vae}) that
will hopefully scale better to natural images.}

\relatedss{Subitizing Datasets}{As seen in Fig. \ref{fig:sub}, the goal of the
\emph{Salient Object Subitizing} (SOS) dataset as defined by
\citet{zhang2016salient} is to clearly show a number of salient objects
that lies within the subitizing range. As other approaches often perform
poor on images with complex backgrounds or with a large number of
objects, \citet{zhang2016salient} also introduce images with no salient
objects, as well as images where the number of salient objects lies
outside of the subitizing range. The SOS dataset was
constructed from an ensemble of other datasets to avoid potential
dataset bias, and contains approximately 14K natural images. \citet{zhang2016salient} also report subitizing performance of multiple classification algorithms on images from the SOS dataset, although none are trained unsupervised.}
%% \vspace*{-4.2pt}
\begin{figure}[H]
\centering
    \subfloat{\includegraphics[height=2cm, width=4.0cm]{s1}}% label zero
    \quad
    \subfloat{\includegraphics[height=2cm, width=4.0cm]{s2}}%
    \quad
    \subfloat{\includegraphics[height=2cm, width=4.0cm]{s3}}%
    \quad
    \\\vspace*{0.9em}\\
    \subfloat{\includegraphics[height=2cm, width=4.0cm]{s0}}%
    \quad
    \subfloat{\includegraphics[height=2cm, width=4.0cm]{s4}}%
    \caption{Example images from the SOS dataset. Each image shows a number of salient objects. The top row of images correspond to class labels 1-3, whereas the other two receive labels 0 and 4\(+\).}
\label{fig:sub}
\end{figure}

\hypertarget{methods}{%
\section{Methods}\label{methods}}

% aside fromm architecture, mention more training params such as latent dimension size, alpha/beta
% preprocessing etc.

\hypertarget{vae}{%
\subsection{Variational Autoencoder}\label{vae}}

VAEs \citep{kingma2013auto} are part of the family of autoencoder algorithms, owing this title
to the majority of their structure consisting of an encoder and a
decoder module \citep{doersch2016tutorial} (see Fig. \ref{fig:ae} for the
schematics of an autoencoder). In an regular autoencoder, the encoder
module learns to map features from data samples \(X \in \mathbb{R}^{n}\)
into latent variables \(z \in \mathbb{R}^{m}\) often so that \(m \ll n\)
and thus performs in dimensionality reduction, while the decoder
function learns to reconstruct latent variables \(z\) into
\(X' \in \mathbb{R}^{n}\) such that \(X'\) matches \(X\) according to
some predefined similarity measure \citep{liou2014autoencoder}. Reducing
the input to be of much lower dimensionality forces the autoencoder to
learn only the most emblematic regularities of the data, as these will
minimize the reconstruction error. The latent space can thus be seen as
an inferred hidden feature representation of the data.

\begin{figure}
\centering
\includegraphics{ae-small.pdf}
\caption{Schematic architecture of an autoencoder. \texttt{X} indicates the input image, that is then encoded by the \texttt{Encoder Network} into a latent vector \texttt{z}. \texttt{z} is decoded back into a reconstruction \texttt{X\textquotesingle} of input \texttt{X} by the \texttt{Decoder Network}.}
\label{fig:ae}
\end{figure}

Where VAEs primarily differ from regular autoencoders is that rather
than directly coding data samples into some feature space, they learn
the parameters of a distribution that represents that feature space.
Therefore, VAEs perform stochastic inference of the underlying
distribution of the input data, instead of only creating some efficient
mapping to a lower dimensionality that simultaneously facilitates
accurate reconstruction. Now provided with statistical knowledge of the
characteristics of the input, VAEs can not only perform reconstruction,
but also generate novel examples that are similar to the input data
based on the inferred statistics. The ability to generate novel examples
makes VAEs a \emph{generative} algorithm.
% Tom: Explain Normal. Define all your symbols. 
%% 	explain what mu and Sigma are?
%% N is a multivariate Gaussian 
%% - yes this is it ðŸŒ¸ (wikipedia)
%% - but where do mu and sigma come from?

The task of the VAE's encoder network is to infer mean and
variance parameters $\mu$ and $\Sigma$ of a multivariate Gaussian distribution of the latent space, with \(\mu\) and \(\Sigma\) arbitrary deterministic functions, such that latent vectors
\(z\) drawn from this multivariate Gaussian \(\mathcal{N}(\mu(X), \Sigma(X))\) facilitate reconstruction of data samples \(X\)
\citep{doersch2016tutorial}. Novel sampled \(z\) vectors can then be fed
into the decoder network as usual. \(\mu(X)\) and \(\Sigma(X)\) are
constrained to roughly follow a unit Gaussian by minimizing the
Kullback-Leibler divergence (denoted as \(\mathcal{KL}\)) between unit Gaussian
\(\mathcal{N}(0, I)\) and
\(\mathcal{N}(\mu(X), \Sigma(X))\), where
\(\mathcal{KL}\) measures the distance between probability
distributions. Normally distributed latent variables capture the
intuition behind generative algorithms that they should support sampling
latent variables that produce reconstructions that are merely
\emph{similar} to the input, and not necessarily accurate copies \citep{doersch2016tutorial}.
Furthermore, optimizing an arbitrary distribution would be intractable,
thus VAEs need to rely on the fact that given a set of normally
distributed variables \(S = \{s_1, ... s_\mathbf{n}\}\) with
\(S \in \mathbb{R}^{\mathbf{n}}\) and any sufficiently complicated
function \(f(s_i)\) (such as a neural network), there exists a mapping
\(f: S \mapsto S'\) from which we can generate any arbitrary
distribution \(P(X) \in \mathbb{R}^{\mathbf{n}}\) with \(S' \sim P(X)\)
\citep{doersch2016tutorial}. \\

\noindent Therefore, the optimization objectives of a VAE become \citep{doersch2016tutorial}:

% distance between the learned probability distribition and the unit Gaussian
\begin{enumerate}[topsep=2pt]
\def\labelenumi{\arabic{enumi}.}
\tightlist 
\item \(\mathcal{KL}\) divergence loss \(\mathcal{L}_{kl}\) between the learned multivariate Gaussian and the unit Gaussian distributions, defined as:
  \[\mathcal{L}_{kl} = \mathcal{KL}\lbrack\mathcal{N}(\mu(X), \Sigma(X)) \vert\vert \mathcal{N}(0, I)\rbrack\]
\item
 Reconstruction loss \(\mathcal{L}_{rec}\). Within visual problems, plain VAEs can for
  example minimize the binary cross entropy (BCE) between \(X\) and
  \(X'\).
\end{enumerate}

\noindent We define the total loss \(\mathcal{L}_{total}\) with weighting parameters \(\alpha\) and \(\beta\) that weigh the relative important of each optimization objective:

\[\mathcal{L}_{total} = \alpha\mathcal{L}_{kl} + \beta\mathcal{L}_{rec} \tag{1}\]

\noindent Objective 1 grants VAEs the ability to generate new samples
from the learned distribution, partly satisfying the constraint outlined
in the introduction whereby visual numerosity related skills are shown to emerge in
generative learning models. To fully satisfy this constraint, the final
architecture uses deep convolutional networks for both the encoder and decoder
module (see Fig. \ref{fig:vae-arch} for the VAE architecture), making the
implementation an hierarchical model. The VAE is trained by reconstructing samples from the hybrid dataset (outlined in section \ref{hybrid}) and SOS dataset until the loss converges. As a VAE's latent space
encodes the most important features of the data, it is hoped the samples
drawn from the encoder provide information regarding it's subitizing
performance (see section \ref{readout}). 

\hypertarget{deep-feature-consistent-perceptual-loss}{%
\subsubsection{Deep Feature Consistent Perceptual
Loss}\label{deep-feature-consistent-perceptual-loss}}

A BCE based reconstruction loss (see objective 2) yielded blurry reconstructions in our experiments, sometimes distorting numerical information by for exampling merging figure-ground organization.
Because the frequently used pixel-by-pixel reconstruction loss measures
in VAEs (such as a BCE based loss) do not necessarily comply with human perceptual similarity
judgments, \citet{hou2017deep} therefore propose optimizing the reconstructions (i.e. \(\mathcal{L}_{rec}\)) with help of the hidden layers of a pretrained deep CNN, a particular advantage of these models being their ability to better capture spatial correlation relative to pixel-by-pixel measurements \citep{hou2017deep}. For example, a pixel-by-pixel based measure yields a high difference between an image and it's slightly shifted version as none of the pixels align anymore, while these two images will be deemed highly similar by humans.
Additionally, CNNs haven proven to model visual characteristics of
images deemed important by humans, given their ability to for example perform
complex image classification tasks \citep{krizhevsky2012imagenet}, which should help our VAE in retaining the most important of information.  The ability
of the proposed \emph{Feature Perceptual Loss} (FPL) to retain spatial
correlation should reduce the noted blurriness
\citep[see for example]{larsen2015autoencoding} of the VAE's reconstructions \citep{hou2017deep}, which is
especially problematic in subitizing tasks since blurring merges objects
which in turn distorts subitizing labels. \citet{hou2017deep} and
present research employ VGG-19 \citep{simonyan2014very} as the
pretrained network \(\Phi\), trained on the large and varied ImageNet
\citep{russakovsky2015imagenet} dataset. FPL requires predefining a set
of layers \(l_i \in L\) from pretrained network \(\Phi\), and works by
minimizing the mean squared error (MSE) between the hidden
representations of input \(x\) and VAE reconstruction \(\bar{x}\) at
every layer \(l_i\). Aside from the \(\mathcal{KL}\) divergence, the
VAE's reconstruction loss given $L$ is now defined as:

\[ \mathcal{L}^{L}_{rec} = \sum_{l \in L}^{} \textrm{MSE}(\Phi(x)^{l}, \Phi(\bar{x})^{l})\]

\noindent The intuition behind FPL is that whatever some hidden layer \(l_i\) of
the VGG-19 network encodes should be retained in the reconstruction
\(\bar{x}\), as the VGG-19 has proven to model important visual
characteristics of a large variety of image types. In
\citet{hou2017deep}'s and our experiments
\(L = \{\texttt{relu1\_1, relu2\_1, relu3\_1\}}\) resulted in the best
reconstructions. 

\hypertarget{hybrid}{%
\subsection{Hybrid Dataset}\label{hybrid}}

% We follow \citet{zhang2016salient} in pre-training our model with
%% synthetic images and later fine-tuning on the SOS dataset.
We follow \citet{zhang2016salient} in pre-training our model with
synthetic images and later fine-tuning on the SOS dataset. However, some
small chances to their synthetic image pre-training setup are proposed.
First, the synthetic dataset is extended with natural images from the
SOS dataset such that the amount of instances per class is equal in every epoch (hopefully reducing problems encountered with class
imbalance, see section \ref{imbalance}). Another
reason for constructing a hybrid dataset was the fact that the
generation process of synthetic images was noted to 1. produce fairly
unrealistic looking examples, and 2. be considerably less suitable than
natural data for supporting subitizing performance
\citep{zhang2016salient}. The second intuition behind this dataset is
thus that the representation of the VAE must always be at least a little
supportive of natural images, instead of settling on some optimum for
synthetic images. The reason for including natural images in addition to synthetic data is that
any tested growth in dataset size during pre-training resulted into
lower losses. 
The sampling ratio (detailed in section \ref{vae-train}) of natural to synthetic images is increased at each epoch. We grow the
original data size by roughly 8 times, pre-training with a total of
80000 hybrid samples per epoch. Testing many different parameters for
the hybrid dataset was not given much priority as the total loss seemed
to shrink with dataset expansion and training and testing a complete model
was time expensive.

Synthetic images are generated by pasting cutout objects from THUS10000
\citep{cheng2015global} onto the SUN background dataset
\citep{xiao2010sun}. The subitizing label is acquired by pasting an
object \(N\) times, with \(N \in \lbrack0, 4\rbrack\). For each paste,
the object is transformed in equivalent manner to
\citet{zhang2016salient}. However, subitizing is noted be more difficult
when objects are superimposed, forcing recourse to external processes as
counting by object enumeration \citep[p.~57.]{dehaene2011number},
implying that significant paste object overlap should be avoided.
\citet{zhang2016salient} avoid object overlap by defining a threshold
\(t \in \lbrack0,1\rbrack\) whereby an object's visible pixels
\(P_{visible}\) and total amount of pixels \(P_{total}\) should satisfy
\(P_{visible} > t * P_{total}\). For reasons given above, we define the
object overlap threshold as \(t = \sum_{n=0}^{N} 0.5 + n_i * 0.1\) with
\(N \in \lbrack0,4\rbrack\) compared to \citet{zhang2016salient}'s
static \(t=0.5\), as VAEs are especially prone produce blurry
reconstructions \citep{hou2017deep, larsen2015autoencoding}, which
requires extra care with overlapping objects as to not distort class
labels. Refer to Fig. \ref{fig:syn} for examples of generated synthetic images.

\begin{figure}
\centering
\includegraphics{syn.png}
\caption{Synthetic images generated for the pre-training stage. Images receive class labels 2, 3 and 4, from left to right. Although unrealistic looking, synthetic data generation counters class imbalance, displays a more extensive distribution of spatial configurations of objects, and familiarizes the model with a larger variety of object and background types. This is especially relevant for subitizing because it is abstract in regard to spatial information and the visual complexity of objects.}
\label{fig:syn}
\end{figure}

%% - [ ] Add training parameters section (hybrid/bezier, data prepossecing, VAE architecture, finetuing)
\newpage
\hypertarget{experiments}{%
\section{Experiments}\label{experiments}}

\hypertarget{vae-train}{%
\subsection{Training the Variational Autoencoder} \label{vae-train}}

\begin{figure}
\centering
\includegraphics[]{vae_arch.pdf}
\caption{Fully convolutional architecture of the encoder (left) and decoder (right) networks from the Variation Autoencoder. Note that this figure was simplified by omitting the reparameterization trick \citep{kingma2013auto}, which enforces that latent samples are not drawn from \(\mathcal{N}(\mu(x), \Sigma(x))\) directly. \texttt{BN} indicates a batch normalisation layer \citep{ioffe2015batch} and \texttt{FC} indicates a fully connected layer. The dimensions before the CNN indicate filter and kernel size, respectively.}
\label{fig:vae-arch}
\end{figure}

\noindent The fully convolutional (see section \ref{related-work}) architecture of our VAE is presented in Fig. \ref{fig:vae-arch}. Subitizing results (Table \ref{tab:sub-performance}) are reported on a second VAE with a slightly different architecture, for reasons outlined in section \ref{subitizing-read-out}.
The pre-training stage consist of feeding the VAE samples from the hybrid dataset (defined in section \ref{hybrid}). The sampling ratio of natural to synthetic images within the hybrid dataset is increased at each epoch, and is defined by the BÃ©zier curve shown in Fig. \ref{fig:bezier}. After the loss converges on the hybrid dataset, the VAE is fine-tuned using only the SOS dataset by freezing the first three convolutional modules of the encoder network shown in Fig. \ref{fig:vae-arch}.  Data augmentation is performed on all sampled images, as the SOS dataset is relatively small in size (see section \ref{imbalance} and \ref{variational-autoencoder-performance}).  Images are first resized to 183x183, after which a crop to 161x161 size is performed at a random location in the image. Additionally, each sample had 50\% probability of being horizontally mirrored. Finally, the reconstructions of the VAE were observed to contain an abundance of primary color, so more complex color reconstruction was enforced by shifting each sample's R, G and B color channels (with values is in range 0-255) by a small value drawn from a normal distribution with \(\mu=0\) and \(\sigma=10\) \citep[the G channel uses \(\sigma=3.5\) because heavy green shifts produced unrealistic samples, possibly because green lighting contributes most to human color intensity perception][]{anderson1996proposal}. The loss weighting parameters from equation (1) are set as \(\alpha=1.0\) and \(\beta=0.03\). Although \citet{hou2017deep} set \(\beta\) significantly closer to \(\alpha\), larger values of \(\beta\) were found to optimize FPL at the expense of the \(\mathcal{KL}\) divergence loss \(\mathcal{L}_{kl}\) under the SOS dataset. The size of the latent dimension is set to 182. The VAE was implemented\footnote{The code of the full implementation is hosted on \href{https://github.com/rien333/numbersense-vae}{Github}.} in PyTorch \citep{paszke2017automatic}. 

\begin{figure}
\centering
%% \includegraphics[height=7.0cm]{bezier.pdf}
\includegraphics{bezier.pdf}
\caption{BÃ©zier curve with parameters
\(u_0=0, u_1=-0.01, u_2=0.02, u_3=1.0\) defining the sampling ratio of natural images to synthetic images at time \(t\). }
\label{fig:bezier}
\end{figure}

\hypertarget{readout}{%
\subsection{Hidden Representation Classifier}\label{readout}}

\hypertarget{classifierarch}{%
\subsubsection{Classifier architecture}\label{classifierarch}}

To assess whether the learned latent space of the VAE showcases the
emergent ability to perform in subitizing, a two layer fully connected
network was fed with latent activation vectors \(z_{X_i}\) created by the
encoder module of the VAE from an image \(X_i\), and a corresponding
subitizing class label \(Y_i\), where \(X_i\) and \(Y_i\) are
respectively an image and class label from the SOS training set. Both
fully connected layers contain 160 neurons. Each of the linear layers is
followed by a batch normalization layer \citep{ioffe2015batch}, a ReLU
activation function and a dropout layer \citep{srivastava2014dropout}. A fully connected
network was chosen because using another connectionist module for read-outs
of the hidden representation heightens the biological plausibility of
the final approach \citep{zorzi2013modeling}. Additionally,
\citet{zorzi2013modeling} note that the appended connectionist
classifier module be conceived of as a cognitive response module
supporting a particular behavioral task, although the main reason behind
training this classifier is to assess the VAE's performance against other
algorithmic data.

\hypertarget{imbalance}{
\subsubsection{Class imbalance}\label{imbalance}}

Class imbalance is a phenomenon encountered in datasets whereby the
number of instances belonging to one or more classes is significantly
higher than the amount of instances belonging to any of the other
classes. Although there is no consensus on an exact definition of what
constitutes a dataset with class imbalance, we follow
\citet{fernandez2013} in that given over-represented class \(c_m\) the
number of instances \(N_{c_i}\) of one the classes \(c_i\) should
satisfy \(N_{c_i} < 0.4 * N_{c_m}\) for a dataset to be considered
imbalanced. For the SOS dataset, \(N_{c_0} = 2596\), \(N_{c_1} = 4854\),
\(N_{c_2} = 1604\), \(N_{c_3} = 1058\) and \(N_{c_4} = 853\), which
implies that \(c_0\) and \(c_1\) are \emph{majority classes}, while the
others should be considered \emph{minority classes}. Most literature
makes a distinction between three general algorithm-agnostic approaches
that tackle class imbalance \citep[for a discussion,
see][]{fernandez2013}. The first two rebalance the class distribution by
altering the amount of examples per class. However, class imbalance can
not only be conceived of in terms of quantitative difference, but also
as qualitative difference, whereby the relative importance of some class
is weighted higher than others (e.g.~in classification relating to
malignant tumors, misclassifying malignant examples as nonmalignant
could be weighted stronger than other misclassifications) Qualitative
difference might be relevant to the SOS dataset, because examples with
overlapping (i.e.~multiple) objects make subitizing inherently more
difficult (see section \ref{hybrid}), and previous results on
subitizing show that some classes are more difficult to classify than
others \citep{zhang2016salient}.

\begin{enumerate}[noitemsep, topsep=0pt]
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  \emph{Oversampling techniques} Oversampling alters the class
  distribution by producing more examples of the minority class, for
  example generating synthetic data that resembles minority examples
  \citep[e.g.][]{he2008adasyn, chawla2002smote}, resulting in a more
  balanced class distribution.
\item
  \emph{Undersampling techniques}. Undersampling balances the class
  distribution by discarding examples from the majority class.
  Elimination of majority class instances can for example ensue by
  removing highly similair instances.
  \citep[e.g.][]{tomek1976two}
\item
  \emph{Cost sensitive techniques.} Cost sensitive learning does not
  alter the distribution of class instances, but penalizes
  misclassification of certain classes. Cost sensitive techniques are
  especially useful for dealing with minority classes that are
  inherently more difficult (or ``costly'') to correctly classify, as
  optimization towards easier classes could minimize cost even in
  quantitatively balanced datasets if the easier classes for example
  require lesser representational resources of the learning model.
\end{enumerate}

\noindent An ensemble of techniques was used to tackle the class imbalance in the
SOS dataset. First, slight random under-sampling with replacement of the
two majority classes (\(c_0\) and \(c_1\)) is performed
\citep[see][]{JMLR:v18:16-365}, reducing their size by
\textasciitilde{}10\%. Furthermore, as in practice many common
sophisticated under- and oversampling techniques (e.g.~data augmentation
or outlier removal, for an overview see \citet{fernandez2013}) proved
largely non-effective, a cost-sensitive class weighting was applied. The
ineffectiveness of quantitative sampling techniques is likely to be caused
by that in addition to the quantitative difference in class examples,
there is also a slight difficulty factor whereby assessing the class of
latent vector \(z\) is significantly if belongs to \(c_2\) or \(c_3\)
versus any other class, for these two classes require rather precise
contours to discern individual objects, even more so with overlapping
objects, while precise contours remain hard for VAEs given their
tendency to produce blurred reconstructions
\citep{larsen2015autoencoding}. The classifier network therefore seems
inclined to put all of its representational power towards the easier
classes, as this will result in a lower total cost, whereby this
inclination will become even stronger as the quantitative class
imbalance grows. The class weights for cost sensitive learning are set
according to the quantitative class imbalance ratio \citep[similar to
section 3.2 in][]{fernandez2013}, but better accuracy was obtained by
slightly altering the relative difference between the weights by raising
them to some power \(n\). In our experiments, \(n=3\) resulted in a
balance between high per class accuracy scores and aforementioned scores
roughly following the same shape as in other algorithms, which hopefully
implies that the classifier is able to generalize in a manner comparable
to previous approaches. For the SOS dataset with random majority class
undersampling, if \(n \gg 3\) the classifier accuracy for the majority
classes shrinks towards chance, and, interestingly, accuracy for the
minority classes becomes comparable to state of the art machine
learning techniques.


\hypertarget{results-discussion}{%
\section{Results \& Discussion}\label{results-discussion}}

\hypertarget{variational-autoencoder-performance}{%
\subsection{Variational Autoencoder
Performance}\label{variational-autoencoder-performance}}

The VAE's loss converged after 102 pre-training epochs, and once more after 39 epochs of fine-tuning solely on the SOS dataset. For the specific purpose of subitizing, we can see that using FPL loss
is beneficial (indeed, when comparing FPL to BCE loss in the classification task described in
section \ref{readout}, the former performed better). For a
comparison between FPL and BCE-based reconstruction measures on the SOS
dataset, refer to Fig. \ref{fig:dfc_comp}. To get an idea of what sort of
properties the latent space of the VAE encodes, see Fig. \ref{fig:latent}.

\begin{figure}
\centering
\includegraphics{dfc_bce.png}
\caption{Comparison between BCE and FPL reconstruction loss measures. The top row consist of original images from the SOS dataset, and the other two rows are reconstructions made by using FPL and BCE loss, respectively. The VAE using BCE shared the training setup of the VAE outlined in section \ref{vae-train}, with the pre-training stage lasting 40 epochs, and the fine-tuning stage lasting 35 epochs. Some of these reconstructions support the claim that using FPL loss retains more numerical information.}
\label{fig:dfc_comp}
\end{figure}

One notable shortcoming of FPL is that although the
layers from VGG-19 represent important visual information, it is a known
fact that the first few layers of deep CNNs only encode simple features
such as edges and lines (i.e they support contours), which are only
combined into more complex features deeper into the network
\citetext{\citealp{liu2017towards}; \citealp[FPL's authors][ note
something similar]{hou2017deep}}. This means that the optimization
objective is somewhat unambitious, in that it will never try to learn
any other visual features \citep[for examples, refer to][Fig.
6.]{liu2017towards} aside from what the set of predefined layers \(L\)
represents. Indeed, although contour reconstruction greatly improved
with FPL, the reconstruction of detail such as facial features shows
less improvement (Fig. \ref{fig:dfc_comp}). While \citet{hou2017deep} show a successful
application of FPL, they might have been unaware of this shortcoming due
to using a more uniform dataset consisting only of centered faces. 

Although the reconstructions show increased quality of contour
reconstruction, there are a few reoccurring visual disparities between
original and reconstruction. First of, novel patterns often emerge in
the reconstructions, possibly caused by a implementational glitch, or a
considerable difference in tested datasets (FPL is frequently paired
with the CelebA \citet{liu2015deep} dataset). Datasets other than the
SOS dataset showed slightly better performance, indicating that the SOS
dataset is either too small, too varied or requires non-standard tweaking
for FPL to work in it's current form. Most of the improvement in more
uniform datasets came from the fact that the VAE learned to create 
local patterns to give the appearance of uniformly colored regions, but
upon closer inspection placed colors in a grid such that they
gave the appearance of just one color, similar to how for example LED
screens function. Another reconstructional problem is that small regions such as details are sometimes left out, which
could possibly distort class labels (for example, figure-ground contrast might decrease when detail is lost).

\begin{figure*}
\centering
\includegraphics[width=\textwidth, height=8cm]{bird_varied_crop.png}
\caption{Reconstructions of the image in the top-left, created by slightly
increasing the response value of the VAE's latent representation \(z\) at different individual dimensions \(z_i\). Some dimensions give
you a slight idea of what types of information they encode (e.g.~a light source at a
location)}
\label{fig:latent}
\end{figure*}

\hypertarget{subitizing-read-out}{%
\subsection{Subitizing Read-Out}\label{subitizing-read-out}}

Accuracy of the \texttt{zclassifier} (i.e.~the classifier described
in section \ref{classifierarch} concerned
with classification of latent activation patterns to subitizing labels)
is reported in Table \ref{tab:sub-performance} over the withheld SOS test set. We report best performance using a slightly different VAE architecture than the one presented in
Fig. \ref{fig:vae-arch} (which scored a mean accuracy of 40.4).
The main difference between the VAE used in this experiment and the one
that is used throughout the rest of this research is the placement of
intermediate fully connected layers (with size 3096) between the latent
representation and the convolutional stacks, as well as utilizing less convolutional filters. It is theorized that although the VAE with a fully convolutional architecture achieved a lower total loss and better reconstructional quality, a more complex model of the latent space does not necessarily facilitate an easier readout, and could possibly even degrade performance given the same classification algorithm (i.e. latent dimensions of the simpler VAE supporting in subitizing now also encode a variety of other information, as seen in Fig. \ref{fig:latent}, adding noise to the classification task). 

\begin{longtable}[]{@{}rllllll@{}}
\caption{Average Precision (\%) of various algorithms classifying subitizing labels to images from the SOS test set. Accuracy scores of algorithms other than our \texttt{zclassifier} were copied over from \citet{zhang2016salient}. For their
implementation, refer to \citet{zhang2016salient}. }\label{tab:sub-performance}\\
\toprule
& 0 & 1 & 2 & 3 & 4+ & mean\tabularnewline
\midrule
\endhead
Chance & 27.5 & 46.5 & 18.6 & 11.7 & 9.7 & 22.8\tabularnewline
SalPry & 46.1 & 65.4 & 32.6 & 15.0 & 10.7 & 34.0\tabularnewline
GIST & 67.4 & 65.0 & 32.3 & 17.5 & 24.7 & 41.4\tabularnewline
\textbf{zclassifier} & 76.0 & 49.0 & 40.0 & 27.0 & 30.0 & 44.4\tabularnewline
SIFT+IVF & 83.0 & 68.1 & 35.1 & 26.6 & 38.1 & 50.1\tabularnewline
CNN\_FT & 93.6 & 93.8 & 75.2 & 58.6 & 71.6 & 78.6\tabularnewline
\bottomrule
\end{longtable}
%% \newpage
The subitizing performance of the VAE is comparable to highest scoring
non-machine learning algorithm, and performs worse overall than the CNNs
trained by \citet{zhang2016salient}. This can be explained by a number
of factors. First of all, the \texttt{CNN\_ft} algorithm used by
\citet{zhang2016salient} has been pretrained on a large, well tested and
more varied dataset, namely ImageNet \citep{russakovsky2015imagenet},
which contains \(\approx 1300\)x more images than the SOS dataset. Additionally, their model
is capable of more complex representations due its depth and the amount
of modules it contains \citep[the applied model from][ uses 22 modules compared
to the 12 in our approach]{szegedy2015going}. Moreover, all their
algorithms are trained in a supervised manner, providing optimization
algorithms such as stochastic gradient descent with a more directly
guided optimization objective, an advantage over present research's
unsupervised training setup.

\hypertarget{qualitative-analysis}{%
\subsection{Qualitative Analysis}\label{qualitative-analysis}}

Artificial and biological neural populations concerned with visual
numerosity support quantitative judgments invariant to object size and,
conversely, some populations detect object size without responding to
quantity, indicating a separate encoding scheme for both properties
\citep{stoianov2012, harvey2013topographic}. Analogously, we tested
whether our VAE's latent representation contained dimensions \(z_i\)
encoding either one of these properties. To test this, we first created
a dataset with synthetic examples containing \(N\) objects
(\(N \in \lbrack 0, 4\rbrack\), with N uniformly distributed over the
dataset) and corresponding cumulative area values \(A\) that those \(N\)
objects occupied (measured in pixels, with \(A\) normally distributed
over the dataset\footnote{An uniform distribution of cumulative area
  might have worked better, but required algorithmic changes to the
  synthetic data generation process that were inhibited by the available time.}). The object overlap threshold was set as $t=1$ for
each example, to reduce noise induced by possible weak encoding
capabilities and reasons outlined in section
\ref{hybrid}. As visualizations showed that each dimension \(z_i\) encodes more
than one type of visual feature (see Fig. \ref{fig:latent}) special care was
undertaken to reduce \(z_i\)'s response variance by only generating data
with 15 randomly sampled objects from the object cut-out set, and one
random background class from the background dataset (performance is
reported on the ``sand deserts'' class, which contains particularly
visually uniform examples). A dimension \(z_i\) is said to be able to
perform as either a numerical or area detector when regressing it's
response over the novel synthetic dataset (\(n=33280\)) supports the
following relationship between normalized variables \(A\) and \(N\)
\citep{stoianov2012}:

\[z_i = \beta_1 \log(N) + \beta_2\log(A) + \varepsilon \quad (\textrm{with }\, N \in [0, 4]) \tag{2} \]

\begin{figure*}[ht]
\centering
\begin{subfigure}{.5\textwidth}
  \centering
  \includegraphics[width=1.0\linewidth, height=14cm]{NN.pdf}
  \caption{}
  \label{fig:zn}
\end{subfigure}%
\begin{subfigure}{.5\textwidth}
  \centering
  \includegraphics[width=1.0\linewidth, height=14cm]{NA.pdf}
   \caption{}
  \label{fig:za}
\end{subfigure}
\caption{(a) shows a typical response profile for a numerosity detector ($R=0.055$).  (b) shows a typical response profile of dimension that encodes cumulative area while being invariant to numerosity information ($R=0.056$). In the top violin plots, dotted lines indicate mean and variance of the response distribution, while thickness indicates frequency. Cumulative area ($A$) was normalized and is displayed across a logarithmic scale. For visual convenience, examples with $A=0$ were shifted next to lowest value of A in the dataset.}
\label{fig:linear}
\end{figure*}


\noindent The regression was accomplished with linear regression algorithms taken
from \citet{newville2016lmfit} (Levenberg--Marquardt proved best). The
criteria set by \citet{stoianov2012} for being a good fit of (2)
are \textbf{1.} the regression explaining at least 10\% of the variance
(\(R^2 \geq 0.1\)) and \textbf{2.}  an ``ideal'' detector of some
property should have a low (\(\mid\beta_i\mid < 0.1\)) regression
coefficient for the complementary property. We slightly altered criteria
\textbf{1} to fit our training setup. The complexity of the SOS
dataset in comparison to the binary images used by \citet{stoianov2012}
requires our model to encode a higher variety of information, meaning
that any fit is going to have more noise as no dimension \(z_i\) has just one
role (see Fig. \ref{fig:latent} for an overview). Moreover, the synthetic data we
use for the regression includes more complex information than the
dataset used by \citet{stoianov2012}. Nevertheless, we still found a
small number of reoccurring detectors of \(A\) and \(N\), with
\(R > 0.065 \pm 0.020\) (all \(z_i\) with \(R > 0.033\) resulted in an
average \(R= 0.07 \pm 0.015\)). Due to randomization in the fitting
process (synthetic examples are randomly generated at each run) the role
distribution varied slightly with each properties being encoded by about
1-2 dimensions, out of the total of 182 (anymore would indicate an
unlikely redundancy, given that the small latent space should provide an
efficient encoding scheme). Some latent dimensions that provide a better
fit of (2) exist, but don't satisfy criteria \textbf{2}. An
interesting note is that whenever the regression showed multiple
dimensions encoding area, they either exhibited positive or negative
responses (i.e.~positive or negative regression coefficients) to area
increase, in accordance with how visual numerosity might rely on a size
normalization signal according to some theories on the
neurocomputational basis for numerosity \citep[for a discussion, see][]{stoianov2012}. A large negative response (in contrast to a
positive) to cumulative area might for example be combined with other
responses in the VAE's decoder network as an indicatory or inhibitory
signal that the area density does not come from just one object, but
from multiple.

Fig. \ref{fig:linear}  provides characteristic response profiles for dimensions
encoding either cumulative area or a subitizing count. For the area
dimension (Fig. \ref{fig:za}), extreme cumulative area samples bend the mean distribution
either upwards or downwards, while the response distribution to
cumulative area for numerosity encoding dimensions stays relatively
centered. The cumulative area detector \(z_{88}\) also shows an increasing response value relative to an increase in cumulative area, especially in comparison to \(z_{77}\). For numerosity dimension \(z_{77}\), Fig. \ref{fig:zn} shows that both the total response and the center of the response distribution increased with
numerosity. In contrast, the dimension that was sensitive cumulative area shows a fairly
static and discontinuous response to changes in subitizing count. With some extra time,
the visual clarity and overall performance of this qualitative analysis
could probably be greatly improved, given that only a short focus on
reducing response variance increased \(R\) by almost a factor of 10 in
some cases.

\hypertarget{conclusion}{%
\section{Conclusion}\label{conclusion}}

We described a setup for training a VAE on a subitizing task, while
satisfying some important biological constraints. A possible consequence
thereof is that our final model showcases properties also found in
biological neuronal networks (as well as other artificial algorithms).
Firstly, an ability to subitize emerged as an implicitly learned skill.
Second of all, the learned encoding scheme indicates support for
encoding numerosities without resorting to counting schemes relying on
cumulative object area, and conversely encodes cumulative area
without using numerosity information, in accordance with findings in artificial models \citep{stoianov2012} and biological neuronal populations \citep{harvey2013topographic, nieder2016neuronal}. However,
more research is needed to assess the coding properties of area and numerisoty detecting dimensions, given that some important properties of the input to this task remain unexhausted, such as visual variation in synthetic images and the distribution of regression variables.
There is also room for improvement in the VAE's reconstructional abilities,
i.e.~efficiency of coding scheme, especially in regard to reconstruction of detail. A possible way to accomplish better coding of details is by experimenting with different optimization objectives \citep[e.g.][]{dosovitskiy2016generating}. An improved encoding might enhance performance in the subitizing classification task. Finally, other promising solutions to the class imbalance problem exist, such as dynamic cost-sensitive techniques, which were not implemented because the internal algorithmic chances they often require are time expensive. This class of algorithms looks promising because a static cost-sensitive approach yielded the best result out of all class imbalance countermeasures. 
Nevertheless, visual numerosity-like skills have emerged during the
training of the VAE, showing the overall ability to perceive numerosity
within the subitizing range without using information provided by visual
features other than quantity. We can thus speak of a fairly abstract
sense of number, as the qualitative analysis of the encoding yielded
promising results over a large variation of images, whereby especially
abstraction in regard to object area has been demonstrated.

\bibliography{thesis}

\end{document}
